# -*- coding: utf-8 -*-
"""Credit Card segmentatiosn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LJ1ds-xsXfs1H2m4WnB-BI_v_CvyR6Kd

### Problem Statement:

This case requires trainees to develop a customer segmentation to define marketing strategy. The sample dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.

## Introduction

Machine Learning is a study of computer algorithms that improve automatically through experience and by the use of data. It is a subfield of artificial intelligence. Machine Learning algorithms build a model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to do so. These ML algorithms are widely used in email spam filtering, computer vision, sales forecasting … etc. where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.

In addition, machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to field of machine learning. And also Data mining is a related field of study, focusing on EDA through unsupervised learning.

Machine Learning approaches are traditionally divided into three broad categories, depending on the nature of the signal or feedback available to the learning system. There are,

Supervised Learning is type of train the Machine Learning algorithms by giving a input features and target in train dataset for to develop a machine learning model, to make predictions.
Unsupervised Learning is a type of train the machine learning algorithms by giving only input features to find the target while in training and deploying the machine learning model.
Reinforcement Learning is a model free train the machine learning algorithms by giving an actions and space in environment and agent objects for to make right predictions in real time.
Scikit-learn is a python module for machine learning built top on Scipy module and is distributed under the 3-Clause BSD License. It is simple and efficient tools for predictive data analysis. It is accessible to everyone, and reusable in various contexts. It was built on Numpy, Scipy and Matplotlib.

Scikit-Learn module mainly used for the Regression, Classification, Clustering, Dimensionality Reduction, Model Selection and Preprocessing of historical data to predict the futures, and classify the data.

System Requirements:

The requirements of the CPU or laptop for to run the project. The following are important requirement to run this project. They are:

    Laptop or CPU with above i3 8th gen processor.
    RAM not less than 8GB
    Python 3.8.8 64 bit & R 4.10 with R Studio  
    Windows 10 64bit
    Required python Packages are listed in `Requirement.txt` file

***Advanced data preparation:*** Build an ‘enriched’ customer profile by deriving “intelligent” KPIs such as:
    
    1- Monthly average purchase and cash advance amount
    
    2- Purchases by type (one-off, installments)
    
    3- Average amount per purchase and cash advance transaction,
    
    4- Limit usage (balance to credit limit ratio),
    
    5- Payments to minimum payments ratio etc.
    
    6- Advanced reporting: Use the derived KPIs to gain insight on the customer profiles.
    
    7- Identification of the relationships/ affinities between services.
    
    8- Clustering: Apply a data reduction technique factor analysis for variable reduction technique and a clustering algorithm  to reveal the behavioural segments of credit card holders
    
    9- Identify cluster characterisitics of the cluster using detailed profiling.
    
    10- Provide the strategic insights and implementation of strategies for given set of cluster characteristics

### DATA DICTIONARY:

***CUST_ID:*** Credit card holder ID

***BALANCE:*** Monthly average balance (based on daily balance averages)

***BALANCE_FREQUENCY:*** Ratio of last 12 months with balance

***PURCHASES:*** Total purchase amount spent during last 12 months

***ONEOFF_PURCHASES:*** Total amount of one-off purchases

***INSTALLMENTS_PURCHASES:*** Total amount of installment purchases

***CASH_ADVANCE:*** Total cash-advance amount

***PURCHASES_ FREQUENCY:*** Frequency of purchases (Percent of months with at least one purchase)

***ONEOFF_PURCHASES_FREQUENCY:*** Frequency of one-off-purchases PURCHASES_INSTALLMENTS_FREQUENCY: Frequency of installment purchases

***CASH_ADVANCE_ FREQUENCY:*** Cash-Advance frequency

***AVERAGE_PURCHASE_TRX:*** Average amount per purchase transaction

***CASH_ADVANCE_TRX:*** Average amount per cash-advance transaction

***PURCHASES_TRX:*** Average amount per purchase transaction

***CREDIT_LIMIT:*** Credit limit

***PAYMENTS:*** Total payments (due amount paid by the customer to decrease their statement balance) in the period

***MINIMUM_PAYMENTS:*** Total minimum payments due in the period.

***PRC_FULL_PAYMEN:*** Percentage of months with full payment of the due statement balance

***TENURE:*** Number of months as a customer

### Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
#Basic python library which need to import
import pandas as pd
import numpy as np

#Date stuff
from datetime import datetime
from datetime import timedelta

#Library for Nice graphing
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.formula.api as sn
# %matplotlib inline

#Library for statistics operation
import scipy.stats as stats

# Date Time library
from datetime import datetime

#Machine learning Library
import statsmodels.api as sm
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# Settings
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

"""### Load data"""

# reading data into dataframe
credit_df= pd.read_csv("credit-card-data.csv")

"""### Information about data set"""

credit_df.head()

credit_df.info()

# Find the total number of missing values in the dataframe
print ("\nMissing values :  ", credit_df.isnull().sum().values.sum())

# printing total numbers of Unique value in the dataframe. 
print ("\nUnique values :  \n",credit_df.nunique())

credit_df.shape

"""---

### Missing Value Treatment
       - Since there are missing values in the data so we are imputing them with median.

---
"""

credit_df.isnull().any()

# CREDIT_LIMIT  and MINIMUM_PAYMENTS has missing values so we need to remove with median.

credit_df['CREDIT_LIMIT'].fillna(credit_df['CREDIT_LIMIT'].median(),inplace=True)

credit_df['CREDIT_LIMIT'].count()


credit_df['MINIMUM_PAYMENTS'].median()
credit_df['MINIMUM_PAYMENTS'].fillna(credit_df['MINIMUM_PAYMENTS'].median(),inplace=True)

# Now again check the missing values.

credit_df.isnull().any()

"""### Deriving New KPI

***1. Monthly average purchase and cash advance amount***

#### Monthly_avg_purchase
"""

credit_df['MONTHLY_AVG_PURCHASE']=credit_df['PURCHASES']/credit_df['TENURE']

"""#### Monthly_cash_advance Amount"""

credit_df['MONTHY_CASH_ADVANCE']=credit_df['CASH_ADVANCE']/credit_df['TENURE']

credit_df[credit_df['ONEOFF_PURCHASES']==0]['ONEOFF_PURCHASES'].count()

"""#### 2- Purchases by type (one-off, installments)

- To find what type of purchases customers are making on credit card
"""

credit_df.loc[:,['ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES']]

"""#### Find customers ONEOFF_PURCHASES and INSTALLMENTS_PURCHASES details"""

credit_df[(credit_df['ONEOFF_PURCHASES']==0) & (credit_df['INSTALLMENTS_PURCHASES']==0)].shape

credit_df[(credit_df['ONEOFF_PURCHASES']>0) & (credit_df['INSTALLMENTS_PURCHASES']>0)].shape

credit_df[(credit_df['ONEOFF_PURCHASES']>0) & (credit_df['INSTALLMENTS_PURCHASES']==0)].shape

credit_df[(credit_df['ONEOFF_PURCHASES']==0) & (credit_df['INSTALLMENTS_PURCHASES']>0)].shape

"""***As per above detail we found out that there are 4 types of purchase behaviour in the data set. So we need to derive a categorical variable based on their behaviour***"""

def purchase(credit):
    if (credit['ONEOFF_PURCHASES']==0) & (credit['INSTALLMENTS_PURCHASES']==0):
        return 'none'
    if (credit['ONEOFF_PURCHASES']>0) & (credit['INSTALLMENTS_PURCHASES']>0):
         return 'both_oneoff_installment'
    if (credit['ONEOFF_PURCHASES']>0) & (credit['INSTALLMENTS_PURCHASES']==0):
        return 'one_off'
    if (credit['ONEOFF_PURCHASES']==0) & (credit['INSTALLMENTS_PURCHASES']>0):
        return 'istallment'

credit_df['PURCHASE_TYPE']=credit_df.apply(purchase,axis=1)

credit_df['PURCHASE_TYPE'].value_counts()

"""#### 4. Limit_usage (balance to credit limit ratio ) credit card utilization
   - Lower value implies cutomers are maintaing thier balance properly. Lower value means good credit score
"""

credit_df['LIMIT_USAGE']=credit_df.apply(lambda x: x['BALANCE']/x['CREDIT_LIMIT'], axis=1)

credit_df['LIMIT_USAGE'].head()

"""#### 5- Payments to minimum payments ratio etc."""

credit_df['PAYMENTS'].isnull().any()
credit_df['MINIMUM_PAYMENTS'].isnull().value_counts()

credit_df['MINIMUM_PAYMENTS'].describe()

credit_df['PAYMENT_TO_MINPAY']=credit_df.apply(lambda x:x['PAYMENTS']/x['MINIMUM_PAYMENTS'],axis=1)

"""####  Extreme value Treatment
- Since there are variables having extreme values so I am doing log-transformation on the dataset to remove outlier effect 
"""

# log tranformation
cr_log=credit_df.drop(['CUST_ID','PURCHASE_TYPE'],axis=1).applymap(lambda x: np.log(x+1))

cr_log.describe()

col=['BALANCE','PURCHASES','CASH_ADVANCE','TENURE','PAYMENTS','MINIMUM_PAYMENTS','PRC_FULL_PAYMENT','CREDIT_LIMIT']
cr_pre=cr_log[[x for x in cr_log.columns if x not in col ]]

cr_pre.columns

cr_log.columns

"""### Insights from KPIs

#### Average payment_minpayment ratio for each purchse type.
"""

x=credit_df.groupby('PURCHASE_TYPE').apply(lambda x: np.mean(x['PAYMENT_TO_MINPAY']))
type(x)
x.values

ax.barh?

fig,ax=plt.subplots()
ax.barh(y=range(len(x)), width=x.values,align='edge')
ax.set(yticks= np.arange(len(x)),yticklabels = x.index);
plt.title('Mean payment_minpayment ratio for each purchse type')

credit_df.describe()

"""#### customers with installment purchases are paying dues """

credit_df.groupby('PURCHASE_TYPE').apply(lambda x: np.mean(x['MONTHY_CASH_ADVANCE'])).plot.barh()

plt.title('Average cash advance taken by customers of different Purchase type : Both, None,Installment,One_Off')

"""#### Customers who don't do either one-off or installment purchases take more cash on advance"""

credit_df.groupby('PURCHASE_TYPE').apply(lambda x: np.mean(x['LIMIT_USAGE'])).plot.barh()

"""#### Original dataset with categorical column converted to number type."""

cre_original=pd.concat([credit_df,pd.get_dummies(credit_df['PURCHASE_TYPE'])],axis=1)

"""### Preparing Machine learning algorithm

***We do have some categorical data which need to convert with the help of dummy creation***
"""

# creating Dummies for categorical variable
cr_pre['PURCHASE_TYPE']=credit_df.loc[:,'PURCHASE_TYPE']
pd.get_dummies(cr_pre['PURCHASE_TYPE'])

"""#### Now merge the created dummy with the original data frame"""

cr_dummy=pd.concat([cr_pre,pd.get_dummies(cr_pre['PURCHASE_TYPE'])],axis=1)

l=['PURCHASE_TYPE']

cr_dummy=cr_dummy.drop(l,axis=1)
cr_dummy.isnull().any()

cr_dummy.info()

cr_dummy.head(3)

sns.heatmap(cr_dummy.corr())

"""<big>
- Heat map shows that many features are co-related so applying dimensionality reduction will help negating multi-colinearity in data
</big>


- Before applying PCA we will standardize data  to avoid effect of scale on our result. Centering and Scaling will make all features with equal weight.

### Standardrizing data 
- To put data on the same scale
"""

from sklearn.preprocessing import  StandardScaler

sc=StandardScaler()

cr_dummy.shape

cr_scaled=sc.fit_transform(cr_dummy)

cr_scaled

"""### Applying PCA

**With the help of principal component analysis we will reduce features**
"""

from sklearn.decomposition import PCA

cr_dummy.shape

#We have 17 features so our n_component will be 17.
pc=PCA(n_components=17)
cr_pca=pc.fit(cr_scaled)

#Lets check if we will take 17 component then how much varience it explain. Ideally it should be 1 i.e 100%
sum(cr_pca.explained_variance_ratio_)

var_ratio={}
for n in range(2,18):
    pc=PCA(n_components=n)
    cr_pca=pc.fit(cr_scaled)
    var_ratio[n]=sum(cr_pca.explained_variance_ratio_)

var_ratio

"""***Since 6 components are explaining about 90% variance so we select 5 components***"""

pc=PCA(n_components=6)

p=pc.fit(cr_scaled)

cr_scaled.shape

p.explained_variance_

np.sum(p.explained_variance_)

np.sum(p.explained_variance_)

var_ratio

pd.Series(var_ratio).plot()

"""***Since 5 components are explaining about 87% variance so we select 5 components***"""

cr_scaled.shape

pc_final=PCA(n_components=6).fit(cr_scaled)

reduced_cr=pc_final.fit_transform(cr_scaled)

dd=pd.DataFrame(reduced_cr)

dd.head()

"""***So initially we had 17 variables now its 5 so our variable go reduced***"""

dd.shape

col_list=cr_dummy.columns

col_list

pd.DataFrame(pc_final.components_.T, columns=['PC_' +str(i) for i in range(6)],index=col_list)

"""#### So above data gave us eigen vector for each component we had all eigen vector value very small we can remove those variable bur in our case its not."""

# Factor Analysis : variance explained by each component- 
pd.Series(pc_final.explained_variance_ratio_,index=['PC_'+ str(i) for i in range(6)])

"""### Clustering

**Based on the intuition on type of purchases made by customers and their distinctive behavior exhibited based on the purchase_type (as visualized above in Insights from KPI) , I am starting with 4 clusters.**
"""

from sklearn.cluster import KMeans

km_4=KMeans(n_clusters=4,random_state=123)

km_4.fit(reduced_cr)

km_4.labels_

pd.Series(km_4.labels_).value_counts()

"""***Here we donot have known k value so we will find the K. To do that we need to take a cluster range between 1 and 21.***

### Identify cluster Error.
"""

cluster_range = range( 1, 21 )
cluster_errors = []

for num_clusters in cluster_range:
    clusters = KMeans( num_clusters )
    clusters.fit( reduced_cr )
    cluster_errors.append( clusters.inertia_ )# clusters.inertia_ is basically cluster error here.

clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors } )

clusters_df[0:21]

# Commented out IPython magic to ensure Python compatibility.
# allow plots to appear in the notebook
# %matplotlib inline
import matplotlib.pyplot as plt
plt.figure(figsize=(12,6))
plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = "o" )

"""***From above graph we will find elbow range. here it is 4,5,6***

### Silhouette Coefficient
"""

from sklearn import metrics

# calculate SC for K=3 through K=12
k_range = range(2, 21)
scores = []
for k in k_range:
    km = KMeans(n_clusters=k, random_state=1)
    km.fit(reduced_cr)
    scores.append(metrics.silhouette_score(reduced_cr, km.labels_))

scores

# plot the results
plt.plot(k_range, scores)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient')
plt.grid(True)

color_map={0:'r',1:'b',2:'g',3:'y'}
label_color=[color_map[l] for l in km_4.labels_]
plt.figure(figsize=(7,7))
plt.scatter(reduced_cr[:,0],reduced_cr[:,1],c=label_color,cmap='Spectral',alpha=0.1)

"""***It is very difficult to draw iddividual plot for cluster, so we will use pair plot which will provide us all graph in one shot. To do that we need to take following steps***"""

df_pair_plot=pd.DataFrame(reduced_cr,columns=['PC_' +str(i) for i in range(6)])

df_pair_plot['Cluster']=km_4.labels_ #Add cluster column in the data frame

df_pair_plot.head()

#pairwise relationship of components on the data
sns.pairplot(df_pair_plot,hue='Cluster', palette= 'Dark2', diag_kind='kde',size=1.85)

"""***It shows that first two components are able to indentify clusters***

#### Now we have done here with priciple component now we need to come bring our original data frame and we will merge the cluster with them. 

***To interprate result we need to use our data frame***
"""

# Key performace variable selection . here i am taking varibales which we will use in derving new KPI. 
#We can take all 17 variables but it will be difficult to interprate.So are are selecting less no of variables.

col_kpi=['PURCHASES_TRX','MONTHLY_AVG_PURCHASE','MONTHY_CASH_ADVANCE','LIMIT_USAGE','CASH_ADVANCE_TRX',
         'PAYMENT_TO_MINPAY','both_oneoff_installment','istallment','one_off','none','CREDIT_LIMIT']

cr_pre.describe()

# Conactenating labels found through Kmeans with data 
cluster_df_4=pd.concat([cre_original[col_kpi],pd.Series(km_4.labels_,name='Cluster_4')],axis=1)

cluster_df_4.head()

# Mean value gives a good indication of the distribution of data. So we are finding mean value for each variable for each cluster
cluster_4=cluster_df_4.groupby('Cluster_4')\
.apply(lambda x: x[col_kpi].mean()).T
cluster_4

fig,ax=plt.subplots(figsize=(15,10))
index=np.arange(len(cluster_4.columns))

cash_advance=np.log(cluster_4.loc['MONTHY_CASH_ADVANCE',:].values)
credit_score=(cluster_4.loc['LIMIT_USAGE',:].values)
purchase= np.log(cluster_4.loc['MONTHLY_AVG_PURCHASE',:].values)
payment=cluster_4.loc['PAYMENT_TO_MINPAY',:].values
installment=cluster_4.loc['istallment',:].values
one_off=cluster_4.loc['one_off',:].values


bar_width=.10
b1=plt.bar(index,cash_advance,color='b',label='Monthly cash advance',width=bar_width)
b2=plt.bar(index+bar_width,credit_score,color='m',label='Credit_score',width=bar_width)
b3=plt.bar(index+2*bar_width,purchase,color='k',label='Avg purchase',width=bar_width)
b4=plt.bar(index+3*bar_width,payment,color='c',label='Payment-minpayment ratio',width=bar_width)
b5=plt.bar(index+4*bar_width,installment,color='r',label='installment',width=bar_width)
b6=plt.bar(index+5*bar_width,one_off,color='g',label='One_off purchase',width=bar_width)

plt.xlabel("Cluster")
plt.title("Insights")
plt.xticks(index + bar_width, ('Cl-0', 'Cl-1', 'Cl-2', 'Cl-3'))
plt.legend()

"""**Insights**

#### Clusters are clearly distinguishing behavior within customers




<big>


- Cluster 2 is the group of customers who have highest Monthly_avg purchases and doing both installment as well as one_off   purchases, have comparatively good credit score. *** This group is about 31% of the total customer base ***
 

 
- cluster 1 is taking maximum advance_cash  and   is paying comparatively less minimum payment and poor credit_score & doing no purchase transaction. *** This group is about 23% of the total customer base ***



- Cluster 0 customers are doing maximum One_Off transactions  and  least payment ratio. *** This group is about 21% of the total customer base ***



- Cluster 3 customers have maximum credit score and  are paying dues and are doing maximum installment purchases. *** This group is about 25% of the total customer base ***


---


"""

# Percentage of each cluster in the total customer base
s=cluster_df_4.groupby('Cluster_4').apply(lambda x: x['Cluster_4'].value_counts())
print (s),'\n'

per=pd.Series((s.values.astype('float')/ cluster_df_4.shape[0])*100,name='Percentage')
print ("Cluster -4 "),'\n'
print (pd.concat([pd.Series(s.values,name='Size'),per],axis=1))

"""#### Finding behaviour with 5 Clusters:"""

km_5=KMeans(n_clusters=5,random_state=123)
km_5=km_5.fit(reduced_cr)
km_5.labels_

pd.Series(km_5.labels_).value_counts()

plt.figure(figsize=(7,7))
plt.scatter(reduced_cr[:,0],reduced_cr[:,1],c=km_5.labels_,cmap='Spectral',alpha=0.5)
plt.xlabel('PC_0')
plt.ylabel('PC_1')

cluster_df_5=pd.concat([cre_original[col_kpi],pd.Series(km_5.labels_,name='Cluster_5')],axis=1)

# Finding Mean of features for each cluster
cluster_df_5.groupby('Cluster_5')\
.apply(lambda x: x[col_kpi].mean()).T

"""### Conclusion With 5 clusters : 

- we have a group of customers (cluster 2) having highest avergae purchases but there is Cluster 4 also having highest cash advance & secong highest purchase behaviour but their type of purchases are same.

- Cluster 0 and Cluster 4 are behaving similar in terms of Credit_limit and have cash transactions is on higher side


---
 
<big>

***So we don't have quite distinguishable characteristics with 5 clusters,***
"""

s1=cluster_df_5.groupby('Cluster_5').apply(lambda x: x['Cluster_5'].value_counts())
print (s1)

# percentage of each cluster

print ("Cluster-5"),'\n'
per_5=pd.Series((s1.values.astype('float')/ cluster_df_5.shape[0])*100,name='Percentage')
print (pd.concat([pd.Series(s1.values,name='Size'),per_5],axis=1))

"""### Finding behavior with 6 clusters"""

km_6=KMeans(n_clusters=6).fit(reduced_cr)
km_6.labels_

color_map={0:'r',1:'b',2:'g',3:'c',4:'m',5:'k'}
label_color=[color_map[l] for l in km_6.labels_]
plt.figure(figsize=(7,7))
plt.scatter(reduced_cr[:,0],reduced_cr[:,1],c=label_color,cmap='Spectral',alpha=0.5)

cluster_df_6 = pd.concat([cre_original[col_kpi],pd.Series(km_6.labels_,name='Cluster_6')],axis=1)

six_cluster=cluster_df_6.groupby('Cluster_6').apply(lambda x: x[col_kpi].mean()).T
six_cluster

fig,ax=plt.subplots(figsize=(15,10))
index=np.arange(len(six_cluster.columns))

cash_advance=np.log(six_cluster.loc['MONTHY_CASH_ADVANCE',:].values)
credit_score=(six_cluster.loc['LIMIT_USAGE',:].values)
purchase= np.log(six_cluster.loc['MONTHLY_AVG_PURCHASE',:].values)
payment=six_cluster.loc['PAYMENT_TO_MINPAY',:].values
installment=six_cluster.loc['istallment',:].values
one_off=six_cluster.loc['one_off',:].values

bar_width=.10
b1=plt.bar(index,cash_advance,color='b',label='Monthly cash advance',width=bar_width)
b2=plt.bar(index+bar_width,credit_score,color='m',label='Credit_score',width=bar_width)
b3=plt.bar(index+2*bar_width,purchase,color='k',label='Avg purchase',width=bar_width)
b4=plt.bar(index+3*bar_width,payment,color='c',label='Payment-minpayment ratio',width=bar_width)
b5=plt.bar(index+4*bar_width,installment,color='r',label='installment',width=bar_width)
b6=plt.bar(index+5*bar_width,one_off,color='g',label='One_off purchase',width=bar_width)

plt.xlabel("Cluster")
plt.title("Insights")
plt.xticks(index + bar_width, ('Cl-0', 'Cl-1', 'Cl-2', 'Cl-3','Cl-4','Cl-5'))

plt.legend()

cash_advance=np.log(six_cluster.loc['MONTHY_CASH_ADVANCE',:].values)
credit_score=list(six_cluster.loc['LIMIT_USAGE',:].values)
cash_advance

"""### Conclusion with  6 clusters:

- Here also groups are overlapping.
     - Cl-0 and Cl-2 behaving same

### Checking performance metrics for Kmeans
- I am validating performance with 2 metrics Calinski harabaz and Silhouette score
"""

from sklearn.metrics import silhouette_score, calinski_harabasz_score

score={}
score_c={}
for n in range(3,10):
    km_score=KMeans(n_clusters=n)
    km_score.fit(reduced_cr)
    score_c[n]=calinski_harabasz_score(reduced_cr,km_score.labels_)
    score[n]=silhouette_score(reduced_cr,km_score.labels_)

pd.Series(score).plot()

pd.Series(score_c).plot()

"""**Performance metrics also suggest that  K-means with 4 cluster is able to show distinguished characteristics of each cluster.**

***Insights with 4 Clusters***



- Cluster 2 is the group of customers who have highest Monthly_avg purchases and doing both installment as well as one_off   purchases, have comparatively good credit score. ***This group is about 31% of the total customer base ***
 

 
- cluster 1 is taking maximum advance_cash  and   is paying comparatively less minimum payment and poor credit_score & doing no purchase transaction. ***This group is about 23% of the total customer base***



- Cluster 0 customers are doing maximum One_Off transactions  and  least payment ratio and credit_score on lower side ***This group is about 21% of the total customer base***



- Cluster 3 customers have maximum credit score and  are paying dues and are doing maximum installment purchases.*** This group is about 25% of the total customer base***

### Marketing Strategy Suggested:

#### a. Group 2
   - They are potential target customers who are paying dues and doing purchases and maintaining comparatively good credit score )
       --    we can increase credit limit or can lower down interest rate
       --    Can be given premium card /loyality cards to increase transactions
       
#### b. Group 1
   - They have poor credit score and taking only cash on advance. We can target them by providing  less interest rate on purchase transaction
   
#### c. Group 0
   - This group is has minimum paying ratio and using card for just oneoff transactions (may be for utility bills only). This group seems to be risky group.
   
#### d. Group 3
  - This group is performing best among all as cutomers are maintaining good credit score and paying dues on time.
      -- Giving rewards point will make them perform more purchases.
"""